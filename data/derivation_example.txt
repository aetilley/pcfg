It all starts with a

Raw corpus:
1) Every cat loves a dog
2) Fido is a cat
3) Fido is a dog
4) Fluffy is a cat
5) Fido loves Fluffy
6) Fluffy sleeps soundly

If we're luckly, our raw corpus has associated

Semantic Data (Tagged and Parsed):

(S (NP (DET Every) (NN cat)) (VP (VT loves) (NP (DET a) (NN dog))))
(S (NP Fido) (VP (VT is) (NP (DET a) (NN cat))))
(S (NP Fido) (VP (VT is) (NP (DET a) (NN dog))))
(S (NP Fluffy) (VP (VT is) (NP (DET a) (NN cat))))
(S (NP Fido) (VP (VT loves) (NP Fluffy)))
(S (NP Fluffy) (VP (VI sleeps) (ADV soundly)))

(the above semantic data is exactly the contents of data/toy_univ_tree.txt)

(If your raw data does not have associated semantic data, you need to either parse the raw data by hand or use another parser.)

Once we have the semantic data, we can compute rule counts line by line:

1) (S, (NP, VP)): 1, (NP, (DET, NN)): 2, (VP, (VT, NP)): 1,
(DET, (Every,)): 1, (NN, (cat,)): 1, (VT, (loves,)): 1, (DET, (a,)): 1, (NN, (dog,)): 1

2) (S, (NP, VP)): 1, (VP, (VT, NP)): 1, (NP, (DET, NN)): 1,
(NP, (Fido,)): 1, (VT, (is,)): 1, (DET, (a,)): 1, (NN, cat): 1

3) (S, (NP, VP)): 1, (VP, (VT, NP)): 1, (NP, (DET, NN)): 1,
(NP, (Fido,)): 1, (VT, (is,)): 1, (DET, (a,)): 1, (NN, dog): 1

4) (S, (NP, VP)): 1, (VP, (VT, NP)): 1, (NP, (DET, NN)): 1,
(NP, (Fluffy,)): 1, (VT, (is,)): 1, (DET, (a,)): 1, (NN, cat): 1

5) (S, (NP, VP)): 1, (VP, (VT, NP)): 1,
(NP, (Fido,)): 1, (VT, (loves,)): 1, (NP, (Fluffy,)): 1

6) (S, (NP, VP)): 1, (VP, (VI, ADV)): 1,
(NP, (Fluffy,)):1, (VI, (sleeps,)): 1, (ADV, (soundly,)): 1

So in total we have

binary rules
(S, (NP, VP)): 6
(NP, (DET, NN)): 5
(VP, (VT, NP)): 5
(VP, (VI, ADV)): 1

and unary rules
(DET, (Every,)): 1
(NN, (cat,)): 3
(VT, (loves,)): 2
(DET, (a,)): 4
(NN, (dog,)): 2
(NP, (Fido,)): 3
(VT, (is,)): 3
(NP, (Fluffy,)): 3
(VI, (sleeps,)): 1
(ADV, (soundly,)): 1


As a UNIV_COUNTS format file this could be

6 S NP VP
5 NP DET NN
5 VP VT NP
1 VP VI ADV
1 DET Every
3 NN cat
2 VT loves
4 DET a
2 NN dog
3 NP Fido
3 VT is
3 NP Fluffy
1 VI sleeps
1 ADV soundly

(This is exactly the contents of the file data/toy_univ_counts.txt.)

Lastly, we may use the rule counts to compute Variable counts and then compute
MLE parameter extimates
for each rule.
For instance, summing over all rules in which NP is the source gives

count(NP) = count(NP, (DET, NN)) + count(NP, (Fido,)) + count(NP, (Fluffy,)) = 5 + 3 + 3 = 11

Since the rule (NP, (DET, NN)) occurs exactly 5 times, we  estimate the parameter
for (NP (DET NN)) to be 5/11 = .454545...
(Think of this as the conditional probability of transitioning to (DET NN) from (given) NP.)
Proceeding in this way, computing sums for all Variables and computing MLE estimates
for all rules in the counts file gives

NP DET NN 0.45454545454545453
S NP VP 1.0
VP VI ADV 0.16666666666666666
VP VT NP 0.8333333333333334
DET Every 0.2
NN cat 0.6
NN dog 0.4
VT is 0.6
NP Fluffy 0.2727272727272727
VI sleeps 1.0
ADV soundly 1.0
DET a 0.8
NP Fido 0.2727272727272727
VT loves 0.4

(This is exactly the contents of the file data/toy_univ_pcfg.txt.)

The train_from_file() method is actually computing this kind of data every time it is called to train
a new PCFG (in the case where the PCFG is trained from a counts file, it must compute MLE parameter
estimates first, and in the case where the PCFG is trained from a tree file,
it must read counts off of the trees. 

Once a PCFG instance is properly trained, it can be used to parse other raw text.

As a special case we could turn our PCFG objects parser onto the raw text from the semantic data
that we used to train the PCFG.  We shouldn't expect that this will alway give the same parse, but it should
often give the same parse especially in the case where we started out with a small, simple treebank.

The reader might have guess correctly that, a PCFG object trained on this last UNIV_PCFG data will
actually parse the raw corpus at the top of this file to the semantic data (treebank) just below it
(use .parse_file(<path_to_parse>, <path_to_output_file>)).

Thus the four files with filenames having prefix "toy" form a cycle: toy_univ_tree.txt, toy_univ_counts.txt,
and toy_univ_pcfg.txt all train the exact same pcfg, and in this case if you apply the resulting pcfg
as a parser to the raw corpus toy_raw.txt the output is exactly toy_univ_tree.txt.


